{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns URLs in the range of years given\n",
    "def get_url(begin_yr, end_yr):\n",
    "    apiURLroot = 'http://api.nytimes.com/svc/archive/v1/'\n",
    "#     apiUrl='http://api.nytimes.com/svc/archive/v1/1945/1.json'\n",
    "    key='api-key=ab651f166e104ed9b22f2cf34bdcdc9b'  # replace with api-kek here\n",
    "    for year in range(begin_yr, end_yr + 1):\n",
    "        for month in range(1,13):\n",
    "            apiUrl = apiURLroot+str(year)+'/'+str(month)+'.json'\n",
    "            link=[apiUrl, key]\n",
    "            ReqUrl='?'.join(link)\n",
    "            yield year,month, ReqUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_news_content(df):\n",
    "    df_news_json_body = json_normalize(df['response.docs'][0])\n",
    "    df_news_frnt_pg = df_news_json_body[df_news_json_body['type_of_material'] == 'Front Page'][df_news_json_body['type_of_material'] == 'Front Page']\n",
    "    df_news_body = df_news_frnt_pg[['pub_date','lead_paragraph']]\n",
    "    df_news_body['para_len'] = df_news_body['lead_paragraph'].apply(lambda(x) : len(str(x)))\n",
    "    df_news_body = df_news_body[df_news_body['para_len'] > 100]\n",
    "    df_news_body['news_body'] =  pd.DataFrame(df_news_body['lead_paragraph'].apply(lambda x : pre_process_text(x)))\n",
    "    return df_news_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(year, month, df):\n",
    "    if not os.path.exists('news_pickle'):\n",
    "        os.makedirs('news_pickle')\n",
    "    file_name = 'news_pickle/' + str(year) + '_' + str(month) + '_' + 'news.pickle'\n",
    "    with open(file_name,'wb') as f:\n",
    "        pickle.dump(df, f,pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "with open('exception_words.txt','r') as f:\n",
    "    exception_words = f.read()\n",
    "    f.close()\n",
    "exception_words = re.split('\\n',exception_words)\n",
    "# print(exception_words)\n",
    "def pre_process_text(text):\n",
    "#     remove punct\n",
    "    text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "#     tokenize\n",
    "    text_tokens = re.split('\\W+',text_no_punct.lower())\n",
    "#     remove stop words\n",
    "    text_no_stop_w = [word for word in text_tokens if word not in stopwords ]\n",
    "#     clean text\n",
    "    text_remv_excp = [word for word in text_no_stop_w if word not in exception_words ]\n",
    "    text_clean = [word for word in text_remv_excp if word != 'nan']\n",
    "#     Lemmatize\n",
    "    text_lemmatized =  \" \".join([wn.lemmatize(word) for word in text_clean])\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use JSON to download the archive for one month as a pandas data-fram\n",
    "def download_content(from_year=1941,to_year=1945):\n",
    "    for year, month, url in get_url(from_year,to_year):\n",
    "        \n",
    "        jstr = urllib2.urlopen(url).read()\n",
    "        ns_js = json.loads(jstr)\n",
    "        \n",
    "        df_news_arch = json_normalize(ns_js)\n",
    "        \n",
    "        df_vect = parse_news_content(df_news_arch)\n",
    "#         Save only the post-processed body of text\n",
    "        save_to_pickle(year, month, df_vect['news_body'])\n",
    "        time.sleep(5)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_lda(from_year=1941,to_year=1945):\n",
    "    df_by_year = None\n",
    "    for year in range(from_year, to_year + 1):\n",
    "        for month in range(1,13):\n",
    "            v_file = 'news_pickle/' + str(year) + '_' + str(month) + '_news.pickle'\n",
    "            data = None\n",
    "            no_of_words = 10\n",
    "            with open(v_file,'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                f.close()\n",
    "            df_data = pd.DataFrame(data)\n",
    "            \n",
    "            vector_tf = TfidfVectorizer(smooth_idf=True)\n",
    "            X_tf = vector_tf.fit_transform(df_data['news_body'])\n",
    "            \n",
    "            lda = LatentDirichletAllocation(\n",
    "                                n_components=1, \n",
    "                                #learning_decay=.6,\n",
    "                                max_iter=5,\n",
    "                                learning_method='batch',\n",
    "                                #learning_offset=30.,\n",
    "                                random_state=99\n",
    "                                )\n",
    "            lda.fit(X_tf)\n",
    "            \n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                topics =  \" \".join([vector_tf.get_feature_names()[i]\n",
    "                for i in topic.argsort()[:-no_of_words - 1:-1]])\n",
    "                \n",
    "            \n",
    "            if df_by_year is None:\n",
    "                df_by_year =  pd.DataFrame(columns = ['year_month','topics'])\n",
    "                df_by_year = df_by_year.append ({ 'year_month' : str(year) + '_' + str(month), 'topics' : topics}, ignore_index=True)\n",
    "            else:\n",
    "                df_by_year = df_by_year.append ({ 'year_month' : str(year) + '_' + str(month), 'topics' : topics}, ignore_index=True)\n",
    "    return df_by_year\n",
    "# execute_lda(1950, 1950).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stargazer/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/stargazer/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "download_content(1941, 1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_year = execute_lda(1941, 1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "df_by_year.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
